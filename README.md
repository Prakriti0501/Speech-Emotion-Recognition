# Speech-Emotion-Recognition
The goal of this project is to teach a computer to understand and recognize human emotions from the sound of someone's voice. For example, the computer should be able to tell if a person sounds happy, sad, or any other emotion, just by listening to their voice.This loads an emotional speech dataset, preprocesses the audio data, creates a neural network model particularly utilizing a Long Short-Term Memory (LSTM) neural network. , and trains it for emotion classification. The provided functions and visualizations help in understanding the characteristics of the audio data and the training process of the model.


#Summary of TESS(Toronto Emotional Speech Test) Dataset:
The TESS dataset is a valuable resource for creating an emotion classifier from audio. What makes it stand out is that it features high-quality audio recordings, specifically focusing on female speakers. This is noteworthy because many other datasets are skewed towards male speakers, introducing a potential imbalance in representation.
The dataset comprises recordings of 200 target words spoken by two actresses, aged 26 and 64, each expressing seven different emotions: anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral. In total, there are 2800 audio files, with each actress and emotion organized into separate folders.
This dataset's uniqueness lies in its quality, female-centric focus, and diverse emotional expressions, making it an excellent choice for training an emotion classifier. Using such a dataset helps ensure the model generalizes well across emotions without overfitting to specific speakers or imbalances in gender representation.
A special thanks to the University of Toronto for curating this valuable dataset!
When discussing challenges faced in building a Speech Emotion Recognition (SER) project, you can emphasize both technical and practical aspects. Here's an example response:
happiness, sadness, anger, fear, disgust, surprise, and neutrality.
